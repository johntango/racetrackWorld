{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce978f4a",
   "metadata": {},
   "source": [
    "Racetrack World Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0266a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fe142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize racetrack environment\n",
    "def create_track(width=10, height=10):\n",
    "    track = np.zeros((height, width))\n",
    "    track[1:-1, 1:-1] = 1  # drivable area\n",
    "    start = (1, 1)\n",
    "    finish = (height - 2, width - 2)\n",
    "    return track, start, finish\n",
    "\n",
    "def reset():\n",
    "    return (1, 1)\n",
    "\n",
    "def step(track, position, action):\n",
    "    moves = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # Up, Down, Left, Right\n",
    "    new_pos = (position[0] + moves[action][0], position[1] + moves[action][1])\n",
    "\n",
    "    if (0 <= new_pos[0] < track.shape[0] and\n",
    "        0 <= new_pos[1] < track.shape[1] and\n",
    "        track[new_pos] == 1):\n",
    "        position = new_pos\n",
    "\n",
    "    done = position == (track.shape[0] - 2, track.shape[1] - 2)\n",
    "    reward = 1 if done else -0.1\n",
    "\n",
    "    return position, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1c3519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(track, position, finish):\n",
    "    track_vis = track.copy()\n",
    "    track_vis[position] = 2\n",
    "    track_vis[finish] = 3\n",
    "    plt.imshow(track_vis, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "# Initialize Q-learning agent\n",
    "def initialize_q_table(track):\n",
    "    return np.zeros((track.shape[0], track.shape[1], 4))\n",
    "\n",
    "def choose_action(q_table, state, epsilon=0.2):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(4)\n",
    "    return np.argmax(q_table[state[0], state[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f22344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(q_table, state, action, reward, next_state, alpha=0.6, gamma=0.9):\n",
    "    predict = q_table[state[0], state[1], action]\n",
    "    target = reward + gamma * np.max(q_table[next_state[0], next_state[1]])\n",
    "    q_table[state[0], state[1], action] += alpha * (target - predict)\n",
    "\n",
    "# Training function\n",
    "def train_agent(episodes=200):\n",
    "    track, start, finish = create_track()\n",
    "    q_table = initialize_q_table(track)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = choose_action(q_table, state)\n",
    "            next_state, reward, done = step(track, state, action)\n",
    "            learn(q_table, state, action, reward, next_state)\n",
    "            state = next_state\n",
    "\n",
    "    return track, q_table, finish\n",
    "\n",
    "# Demonstration\n",
    "track, q_table, finish = train_agent()\n",
    "\n",
    "# Visualize trained agent\n",
    "state = reset()\n",
    "done = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5672c118",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while not done:\n",
    "    render(track, state, finish)\n",
    "    action = choose_action(q_table, state)\n",
    "    state, _, done = step(track, state, action)\n",
    "\n",
    "render(track, state, finish)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
